# -*- coding: utf-8 -*-
"""model-test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kBnDg-Gw3v0gLbT8pbqGY7hsrW1-m2sZ
"""

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
import pickle

from urllib.request import urlopen

# Replace 'your_model_url.h5' with the actual URL of your model file
model_url = 'https://github.com/techtwists/godot-code/raw/main/model.h5'

# Download the model file
model_file = urlopen(model_url)

# Save the model file locally
with open('downloaded_model.h5', 'wb') as f:
    f.write(model_file.read())

# Load the model
loaded_model = load_model('downloaded_model.h5')

loaded_model.summary()

# Replace 'your_model_url.h5' with the actual URL of your model file
input_token_url = 'https://github.com/techtwists/godot-code/raw/main/tokenizer_input.pkl'

# Download the model file
input_token = urlopen(input_token_url)

# Save the model file locally
with open('tokenizer_input.pkl', 'wb') as f:
    f.write(input_token.read())

# Replace 'your_model_url.h5' with the actual URL of your model file
output_token_url = 'https://github.com/techtwists/godot-code/raw/main/tokenizer_output.pkl'

# Download the model file
output_token = urlopen(output_token_url)

# Save the model file locally
with open('tokenizer_output.pkl', 'wb') as f:
    f.write(output_token.read())

# Load the tokenizers
with open('tokenizer_input.pkl', 'rb') as f:
    tokenizer_input = pickle.load(f)

with open('tokenizer_output.pkl', 'rb') as f:
    tokenizer_output = pickle.load(f)

# Function to generate predictions
def generate_output_sequence(input_sequence, model, tokenizer_input, tokenizer_output, max_input_length, max_output_length):
    # Tokenize the input sequence
    input_sequence = tokenizer_input.texts_to_sequences([input_sequence])
    input_sequence = pad_sequences(input_sequence, maxlen=max_input_length, padding='post')

    # Initialize the decoder input with a start token
    target_seq = np.zeros((1, max_output_length))
    start_token = tokenizer_output.word_index.get('<start>')

    target_seq[0, 0] = start_token

    # Generate the output sequence
    for i in range(1, max_output_length):
        output_probs = model.predict([input_sequence, target_seq])[0, i - 1, :]
        predicted_token_index = np.argmax(output_probs)
        target_seq[0, i] = predicted_token_index

        end_token_index = tokenizer_output.word_index.get('<end>')
        if end_token_index is not None and predicted_token_index == end_token_index:
            break

    # Convert the predicted sequence back to text
    predicted_sequence = [tokenizer_output.index_word[index] for index in target_seq[0] if index > 0]
    return ' '.join(predicted_sequence)

# Example usage
max_input_length = 80
max_output_length = 187
input_instruction = "hello"
predicted_output = generate_output_sequence(input_instruction, loaded_model, tokenizer_input, tokenizer_output, max_input_length, max_output_length)
print("Predicted Output:", predicted_output)